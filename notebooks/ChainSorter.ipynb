{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "61f6042b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from google.cloud import storage\n",
    "import os\n",
    "import json\n",
    "import subprocess \n",
    "from pathlib import Path\n",
    "from glob import glob \n",
    "import time\n",
    "import sys\n",
    "import pyrebase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6c05a20e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running command: gsutil -m rsync -r gs://mina_mainnet_blocks mina_mainnet_blocks\n",
      "22937 Blocks Synchronized in 159.5471749305725 seconds\n"
     ]
    }
   ],
   "source": [
    "# Synchronize local block collection with Google Cloud bucket\n",
    "bucket_name = \"mina_mainnet_blocks\"\n",
    "output_dir = \"mina_mainnet_blocks\"\n",
    "def download_blocks():\n",
    "    Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
    "    command = f\"gsutil -m rsync -r gs://{bucket_name} {output_dir}\"\n",
    "    print(f\"Running command: {command}\")\n",
    "    sync_blocks = subprocess.run(command.split(), stdout=subprocess.PIPE, text=True)\n",
    "\n",
    "    return glob(output_dir + \"/*\")\n",
    "\n",
    "\n",
    "start = time.time()\n",
    "files = download_blocks()\n",
    "end = time.time()\n",
    "print(f\"{len(files)} Blocks Synchronized in {end-start} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d94ff6b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsed 22647 Blocks in 127.98336863517761 seconds\n"
     ]
    }
   ],
   "source": [
    "# Load into memory blocks\n",
    "# This will also trim down block content\n",
    "# If you're looking for something in a block and can't find it, this is (probably) why\n",
    "def load_blocks(block_file_list):\n",
    "    blocks = {}\n",
    "    \n",
    "    for file in block_file_list:\n",
    "        state_hash = file.split(\"-\")[1].split(\".\")[0]\n",
    "        with open(file, \"r\", encoding = \"ISO-8859-1\") as json_file:\n",
    "            contents = json_file.read()\n",
    "            block = json.loads(contents)\n",
    "            \n",
    "            # Data is trimmed here, key structure left intact \n",
    "            # Kinda messy, but saves a LOT of memory and a fair bit of process time\n",
    "            timestamp = block[\"scheduled_time\"]\n",
    "            previous_state_hash = block[\"protocol_state\"][\"previous_state_hash\"]\n",
    "            creator = block[\"protocol_state\"][\"body\"][\"consensus_state\"][\"block_creator\"]\n",
    "            height = block[\"protocol_state\"][\"body\"][\"consensus_state\"][\"global_slot_since_genesis\"]\n",
    "            charged = block[\"protocol_state\"][\"body\"][\"consensus_state\"][\"supercharge_coinbase\"]\n",
    "            block.clear()\n",
    "            block[\"protocol_state\"] = {}\n",
    "            block[\"protocol_state\"][\"body\"] = {}\n",
    "            block[\"protocol_state\"][\"body\"][\"consensus_state\"] = {}\n",
    "            block[\"scheduled_time\"] = timestamp\n",
    "            block[\"protocol_state\"][\"previous_state_hash\"] = previous_state_hash\n",
    "            block[\"protocol_state\"][\"body\"][\"consensus_state\"][\"block_creator\"] = creator\n",
    "            block[\"protocol_state\"][\"body\"][\"consensus_state\"][\"global_slot_since_genesis\"] = height\n",
    "            block[\"protocol_state\"][\"body\"][\"consensus_state\"][\"supercharge_coinbase\"] = charged\n",
    "\n",
    "            blocks[state_hash] = block\n",
    "            \n",
    "    return blocks\n",
    "\n",
    "start = time.time()\n",
    "blocks = load_blocks(files)\n",
    "end = time.time()\n",
    "print(f\"Parsed {len(blocks.keys())} Blocks in {end-start} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b8fcf9b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate node mapping\n",
    "from graph_tool import Graph\n",
    "from graph_tool.draw import graph_draw, graphviz_draw, arf_layout, fruchterman_reingold_layout, sfdp_layout\n",
    "g = Graph()\n",
    "vertices = {}\n",
    "# Retain hash data in vertex for later lookup\n",
    "vertex_hash = g.new_vertex_property('string')\n",
    "\n",
    "for state_hash in __builtins__.list(blocks.keys()): \n",
    "    block = blocks[state_hash]\n",
    "    previous_state_hash = block[\"protocol_state\"][\"previous_state_hash\"]\n",
    "    # Add a node for this block\n",
    "    if state_hash not in vertices:\n",
    "        vertices[state_hash] = g.add_vertex()\n",
    "        vertex_hash[vertices[state_hash]] = state_hash\n",
    "    if previous_state_hash not in vertices: \n",
    "        vertices[previous_state_hash] = g.add_vertex()\n",
    "        vertex_hash[vertices[previous_state_hash]] = previous_state_hash\n",
    "    g.add_edge(vertices[state_hash], vertices[previous_state_hash])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "81a47e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine node coordinates for visual display\n",
    "# Force-Directed Layout\n",
    "pos = sfdp_layout(g, p=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "42c8c17b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine where the \"endpoint\" vertices are in our node network (forks + canonical endpoint)\n",
    "in_degrees = g.get_in_degrees(g.get_vertices())\n",
    "endpoints = set()\n",
    "for vertex, in_degree in enumerate(in_degrees):\n",
    "    if in_degree == 0:\n",
    "        endpoints.add(vertex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0c604d8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found canonical chain using blockchain data at Node 7578 in 0.028016328811645508 seconds\n"
     ]
    }
   ],
   "source": [
    "# For each endpoint, do checks to determine which is canonical\n",
    "canonical = ('',0)\n",
    "\n",
    "# Check #1: blockHeight comparison via block data\n",
    "def heightCompare(blockList):\n",
    "    longest = 0\n",
    "    for index, block in enumerate(blockList):\n",
    "        length = int(blocks[vertex_hash[block]][\"protocol_state\"][\"body\"][\"consensus_state\"][\"global_slot_since_genesis\"])\n",
    "        if (length > longest):\n",
    "            longest = length\n",
    "            chainResult = (vertex_hash[block], block)\n",
    "    return chainResult\n",
    "        \n",
    "# Check #2: Manually counting back each endpoint through the blockchain to find longest chain :V\n",
    "# Essentually manually verifying the data stored in the blockchain - guaranteed to work, but SLOW\n",
    "def crawl(block, count):\n",
    "    count += 1\n",
    "    node = g.vertex(block)\n",
    "    if(sum(1 for neighbor in node.out_neighbors()) == 0):\n",
    "        return count\n",
    "    for neighbor in node.out_neighbors():\n",
    "        return crawl(neighbor, count)\n",
    "\n",
    "def manualCompare(blockList):\n",
    "    longest = 0\n",
    "    for block in blockList:\n",
    "        length = crawl(block, 0)\n",
    "        if(length > longest):\n",
    "            longest = length\n",
    "            manualResult = (vertex_hash[block], block)\n",
    "    return manualResult\n",
    "\n",
    "# Check #3: \n",
    "\n",
    "start = time.time()\n",
    "canonical = heightCompare(endpoints)\n",
    "end = time.time()\n",
    "print(f\"Found canonical chain using blockchain data at Node {canonical[1]} in {end-start} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "48d71567",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recursively painted canonical chain in 0.27088212966918945 seconds\n"
     ]
    }
   ],
   "source": [
    "# Color our graph in a visually pleasing manner\n",
    "# This is also used to paint nodes as \"canonical\" for use in sorting\n",
    "blue = (0,0,1,1)\n",
    "red = (1,0,0,1)\n",
    "green = (0,1,0,1)\n",
    "vertex_canon = g.new_vertex_property('bool')\n",
    "vertex_color = g.new_vertex_property('vector<double>')\n",
    "g.vertex_properties['vertex_canon'] = vertex_canon\n",
    "g.vertex_properties['vertex_color'] = vertex_color\n",
    "\n",
    "# Starts at the canonical node and paints anything in the canonical chain green\n",
    "def colorCrawl(index):\n",
    "    node = g.vertex(index)\n",
    "    vertex_color[node] = green\n",
    "    vertex_canon[node] = True\n",
    "    if(sum(1 for neighbor in node.out_neighbors()) == 0):\n",
    "        vertex_color[node] = blue # mark genesis node as blue\n",
    "        return 0\n",
    "    for neighbor in node.out_neighbors():\n",
    "        return colorCrawl(neighbor)\n",
    "\n",
    "# Paint every node red, then paint canonical nodes green\n",
    "for v in g.vertices():\n",
    "    vertex_color[v] = red\n",
    "    vertex_canon[v] = False\n",
    "    \n",
    "# This is a recursive crawl - it's a BIG recursive crawl\n",
    "# ENTERING THE DANGER ZONE\n",
    "sys.setrecursionlimit(1000000) # Number must be higher than the canonical chain at all times\n",
    "\n",
    "start = time.time()\n",
    "colorCrawl(canonical[1])\n",
    "end = time.time()\n",
    "print(f\"Recursively painted canonical chain in {end-start} seconds\")\n",
    "\n",
    "# Reset recursion limit\n",
    "sys.setrecursionlimit(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8ce0e41c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 9972 forks total, 69 of which are from the past week.\n"
     ]
    }
   ],
   "source": [
    "# Sort non-canonical chains into an array of forks for analysis\n",
    "# 'forks' contains ALL forks\n",
    "# 'shame' contains only forks longer than 5 blocks within the past seven days\n",
    "# Remember, all data can be retrieved for a given block in a fork by calling on the list 'blocks'\n",
    "forks = []\n",
    "shame = []\n",
    "last_week = (int(time.time())-604800)*1000\n",
    "       \n",
    "def forkCrawl(block, container):\n",
    "    node = g.vertex(block)\n",
    "    if vertex_canon[node]:\n",
    "        forks.append(container)\n",
    "        return 0\n",
    "    else:\n",
    "        container.append(vertex_hash[node])\n",
    "        for neighbor in node.out_neighbors():\n",
    "            return forkCrawl(neighbor, container)\n",
    "\n",
    "for vertex in endpoints:\n",
    "    container = []\n",
    "    forkCrawl(vertex, container)\n",
    "    \n",
    "for fork in forks:\n",
    "    for block in fork:\n",
    "        if (int(blocks[block][\"scheduled_time\"]) > last_week and len(fork) > 5):\n",
    "            shame.append(fork)\n",
    "\n",
    "print(f\"Processed {len(forks)} forks total, {len(shame)} of which are from the past week.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "db8c1589",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metadata calculations/analysis\n",
    "# We now have our forks all nicely organized, and can gather metadata properly now\n",
    "# Shove it in an array to send to database in next cell\n",
    "staging = []\n",
    "\n",
    "# Sort every single fork for database update\n",
    "for fork in forks:\n",
    "    forkdata = {}\n",
    "    forkdata[\"length\"] = len(fork)\n",
    "    forkdata[\"blocks\"] = []\n",
    "    forkdata[\"creators\"] = []\n",
    "    forkdata[\"rewards\"] = 0\n",
    "    forkdata[\"latest\"] = ['',0]\n",
    "    for block in fork:\n",
    "        # Blocks and Creators share an index. Block at index 0 was created by Creator at index 0, etc.\n",
    "        forkdata[\"blocks\"].append(block)\n",
    "        forkdata[\"creators\"].append(blocks[block][\"protocol_state\"][\"body\"][\"consensus_state\"][\"block_creator\"])\n",
    "        if blocks[block][\"protocol_state\"][\"body\"][\"consensus_state\"][\"supercharge_coinbase\"]:\n",
    "            forkdata[\"rewards\"] += 1440\n",
    "        else:\n",
    "            forkdata[\"rewards\"] += 720\n",
    "        # Get most recent fork and use its hash + timestamp for the \"ID\"\n",
    "        if int(blocks[block][\"scheduled_time\"]) > int(forkdata[\"latest\"][1]):\n",
    "            forkdata[\"latest\"] = [block, blocks[block][\"scheduled_time\"]]\n",
    "                \n",
    "    staging.append(forkdata)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2f1cab34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pruned 0 deprecated forks from the database in 8.08246922492981 seconds.\n",
      "Added 0 new forks to the database in 9.898189783096313 seconds.\n"
     ]
    }
   ],
   "source": [
    "# Store all Metadata to Firebase DB:\n",
    "fireconfig = {\n",
    "  \"apiKey\": APIKEY,\n",
    "  \"authDomain\": AUTHDOMAIN,\n",
    "  \"databaseURL\": DATABASEURL,\n",
    "  \"storageBucket\": BUCKET\n",
    "}\n",
    "firebase = pyrebase.initialize_app(fireconfig)\n",
    "auth = firebase.auth()\n",
    "user = auth.sign_in_with_email_and_password(USERNAME, PASSWORD)\n",
    "user = auth.refresh(user['refreshToken'])\n",
    "db = firebase.database()\n",
    "db_json = db.child(\"forks\").get(user[\"idToken\"]).val()\n",
    "\n",
    "# Only applies if the database is uninitialized/doesn't exist\n",
    "# Directly populate the database with all data in the chain\n",
    "if db_json is None:\n",
    "    for fork in staging:\n",
    "        db.child(\"forks\").push(fork, user[\"idToken\"])\n",
    "    db_json = db.child(\"forks\").get(user[\"idToken\"]).val()\n",
    "    \n",
    "start = time.time()\n",
    "initdb = len(db_json)\n",
    "\n",
    "# Iterate through database, pruning deprecated content as we go\n",
    "for unique in db_json:\n",
    "    prune = True\n",
    "    for fork in staging:\n",
    "        if db_json[unique][\"latest\"] == fork[\"latest\"]:\n",
    "            prune = False\n",
    "            break\n",
    "    if prune:\n",
    "        db.child(\"forks\").child(unique).remove(user[\"idToken\"])\n",
    "\n",
    "# Fluff for a nice printout, optional\n",
    "db_json = db.child(\"forks\").get(user[\"idToken\"]).val()\n",
    "middb = len(db_json)\n",
    "end = time.time()\n",
    "print(f\"Pruned {initdb-middb} deprecated forks from the database in {end-start} seconds.\")\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "# Iterate through forks, adding new content to the database as we go\n",
    "for fork in staging:\n",
    "    new_fork = True\n",
    "    for unique in db_json:\n",
    "        if fork[\"latest\"] == db_json[unique][\"latest\"]:\n",
    "            new_fork = False\n",
    "            break\n",
    "    if new_fork:\n",
    "        db.child(\"forks\").push(fork, user[\"idToken\"])\n",
    "        \n",
    "# More fluff\n",
    "db_json = db.child(\"forks\").get(user[\"idToken\"]).val()\n",
    "finaldb = len(db_json)\n",
    "end = time.time()\n",
    "print(f\"Added {(finaldb-middb)-(initdb-middb)} new forks to the database in {end-start} seconds.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
